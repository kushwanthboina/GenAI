{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, getpass\n",
    "from dotenv import load_dotenv \n",
    "load_dotenv(\".env\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "_set_env(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "from langchain_core.tools import tool\n",
    "from langchain_groq import ChatGroq\n",
    "from langgraph.graph import MessagesState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherResponse(BaseModel):\n",
    "    \"\"\" Respond to the user with this\"\"\"\n",
    "\n",
    "    temperature: float = Field(description='The temperature in fahrenheit')\n",
    "    wind_direction: str = Field(description='The direction of the wind in abbreviated form')\n",
    "    wind_speed: float = Field(description='The Speed of the wind in km/h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inherit 'messages' key from MessageState, which is a list of chat messages\n",
    "class AgentState(MessagesState):\n",
    "    # Final structured response from the agent\n",
    "    final_response: WeatherResponse\n",
    "\n",
    "@tool\n",
    "def get_weather(city: Literal['nyc','sf']):\n",
    "    \"\"\" Use this to get weather information\"\"\"\n",
    "\n",
    "    if city == 'nyc':\n",
    "        return 'It is cloudy in NYC, with 5 mph winds in the North-East direction and a temperature of 70 degrees'\n",
    "    elif city == 'sf':\n",
    "        return \"It is 75 degress and sunny in SF, with 3 ph winds in the South-East direction\"\n",
    "    else:\n",
    "        raise AssertionError(\"Unkown city\")\n",
    "    \n",
    "tools = [get_weather]\n",
    "\n",
    "model = ChatGroq(model='llama3-8b-8192')\n",
    "\n",
    "model_with_tools = model.bind_tools(tools)\n",
    "model_with_structured_output = model.with_structured_output(WeatherResponse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1 single LLM option"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "tools = [get_weather, WeatherResponse]\n",
    "\n",
    "#Force the model to use tools by passing tool_choice = 'any'\n",
    "\n",
    "model_with_response_tool = model.bind_tools(tools,\n",
    "                                            tool_choice='any')\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: AgentState):\n",
    "    response = model_with_response_tool.invoke(state['messages'])\n",
    "\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {'messages': [response]}\n",
    "\n",
    "# Define the function that reponds to user\n",
    "def respond(state: AgentState):\n",
    "    # Construct the final answer from the arguments of the last tool call\n",
    "    weather_tool_call = state['messages'][-1].tool_calls[0]\n",
    "    response = WeatherResponse(**weather_tool_call['args'])\n",
    "\n",
    "    #since we are using tool calling to return structured output\n",
    "    # we need to add a tool message corresponding to the WeatherResponse tool call,\n",
    "    # This is due to LLM providers' requirement that AI messages with tool calls\n",
    "    # need to be follwed by a tool message for each tool call\n",
    "\n",
    "    tool_message = {\n",
    "        'type' : 'tool',\n",
    "        'content': 'Here is your structured response',\n",
    "        'tool_call_id' : weather_tool_call['id'],\n",
    "    }\n",
    "\n",
    "    # We return the final answer\n",
    "    return {'final_response': response,\n",
    "            'messages' : [tool_message]}\n",
    "\n",
    "def should_continue(state: AgentState):\n",
    "    message = state['messages']\n",
    "    last_message = message[-1]\n",
    "    # If there is only tool call and it is the response tool call we respond to the user\n",
    "\n",
    "    if (\n",
    "        len(last_message.tool_calls) == 1\n",
    "        and \n",
    "        last_message.tool_calls[0]['name'] == 'WeatherResponse'\n",
    "    ):\n",
    "        return 'respond'\n",
    "    # Otherwise we will use the tool node again\n",
    "    else:\n",
    "        return 'continue'\n",
    "    \n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(\"agent\", call_model) \n",
    "workflow.add_node(\"respond\",respond)\n",
    "workflow.add_node('tools', ToolNode(tools))\n",
    "\n",
    "# Set the entrypoint as 'agent'\n",
    "# This means that this node is the first one called\n",
    "workflow.set_entry_point('agent')\n",
    "\n",
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    'agent',\n",
    "    should_continue,\n",
    "    {\n",
    "        'continue': 'tools',\n",
    "        'respond' : 'respond'\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge('tools','agent')\n",
    "workflow.add_edge('respond',END)\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = graph.invoke(input={\n",
    "                            'messages': [('human',\n",
    "                                        \"what's the weather in SF\")]\n",
    "                            })['final_response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WeatherResponse(temperature=75.0, wind_direction='S-E', wind_speed=3.0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: 2 LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: AgentState):\n",
    "    response = model_with_tools.invoke(state[\"messages\"])\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "# Define the function that responds to the user\n",
    "def respond(state: AgentState):\n",
    "    # We call the model with structured output in order to return the same format to the user every time\n",
    "    # state['messages'][-2] is the last ToolMessage in the convo, which we convert to a HumanMessage for the model to use\n",
    "    # We could also pass the entire chat history, but this saves tokens since all we care to structure is the output of the tool\n",
    "    response = model_with_structured_output.invoke(\n",
    "        [HumanMessage(content=state[\"messages\"][-2].content)]\n",
    "    )\n",
    "    # We return the final answer\n",
    "    return {\"final_response\": response}\n",
    "\n",
    "\n",
    "# Define the function that determines whether to continue or not\n",
    "def should_continue(state: AgentState):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    # If there is no function call, then we respond to the user\n",
    "    if not last_message.tool_calls:\n",
    "        return \"respond\"\n",
    "    # Otherwise if there is, we continue\n",
    "    else:\n",
    "        return \"continue\"\n",
    "\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"respond\", respond)\n",
    "workflow.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "# Set the entrypoint as `agent`\n",
    "# This means that this node is the first one called\n",
    "workflow.set_entry_point(\"agent\")\n",
    "\n",
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"continue\": \"tools\",\n",
    "        \"respond\": \"respond\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"tools\", \"agent\")\n",
    "workflow.add_edge(\"respond\", END)\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = graph.invoke(input={\"messages\": [(\"human\", \"what's the weather in SF?\")]})[\n",
    "    \"final_response\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WeatherResponse(temperature=75.0, wind_direction='S-E', wind_speed=3.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
